#!/usr/bin/env python
# coding: utf-8

# In[9]:


import streamlit as st
from PIL import Image, UnidentifiedImageError
import requests
import matplotlib.pyplot as plt
import torch.nn as nn
from pylab import imshow
import matplotlib as mpl
import numpy as np
import torch
import cv2
import pandas as pd
import os

from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation
from transformers import AutoImageProcessor, DetrForObjectDetection
# segmentation
processor_seg = SegformerImageProcessor.from_pretrained("mattmdjaga/segformer_b2_clothes")
model_seg = AutoModelForSemanticSegmentation.from_pretrained("mattmdjaga/segformer_b2_clothes")
#object detection
processor_obj = AutoImageProcessor.from_pretrained("facebook/detr-resnet-50")
model_obj = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")



### INTRO ###
st.header('ğŸ‘š ì˜¤ëŠ˜ ë­ì…ì§€?! ğŸ‘•')
st.markdown('ğŸš¨ **ì„¤ë§ˆ ë„ˆ ì§€ê¸ˆ.. ê·¸ë ‡ê²Œ ì…ê³  ë‚˜ê°€ê²Œ?** ğŸš¨')
st.markdown(' **íŒ¨ì…˜ì„¼ìŠ¤ê°€ 2% ë¶€ì¡±í•œ ë‹¹ì‹ ì„ ìœ„í•´ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤!** ì‚¬ì§„ ì´ë¯¸ì§€ë§Œ ì…ë ¥í•˜ë©´, ìš”ì¦˜ íŠ¸ë Œë””í•œ ìŠ¤íƒ€ì¼ê³¼ ì—¬ëŸ¬ë¶„ì˜ TPOë¥¼ ê³ ë ¤í•˜ì—¬ ì½”ë””ë¥¼ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤. ë¬´ì‹ ì‚¬ì™€ ì˜¨ë”ë£©ì˜ íŒ¨ì…”ë‹ˆìŠ¤íƒ€ë“¤ì˜ ì½”ë””ë¥¼ ì§€ê¸ˆ ë°”ë¡œ ì°¸ê³ í•´ë³´ì„¸ìš”! ')
st.image('./intro_img/fashionista.jpg')

st.markdown('--------------------------------------------------------------------------------------')
st.header('PROCESS')
st.image('./intro_img/process.png')
st.markdown('--------------------------------------------------------------------------------------')



### INPUT ###
# # ì˜ë¥˜ ì´ë¯¸ì§€ ì—…ë¡œë“œ
# input_image = st.file_uploader("ì˜ë¥˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”. (ë°°ê²½ì´ ê¹”ë”í•œ ì‚¬ì§„ì´ë¼ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤!)", type=['png', 'jpg', 'jpeg'])
# # ì…ë ¥ë°›ì€ ì˜ë¥˜ ì´ë¯¸ì§€ ì¹´í…Œê³ ë¦¬ ì„ íƒ
# input_cat = st.selectbox(
#     'ê·€í•˜ê°€ ì—…ë¡œë“œí•œ ì˜ë¥˜ ì´ë¯¸ì§€ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ê³¨ë¼ì£¼ì„¸ìš”.',
#     ('top', 'bottom', 'shoes', 'hat', 'sunglasses', 'scarf', 'bag', 'belt'))
# st.write('You selected:', input)
# # ì¶”ì²œë°›ê³  ì‹¶ì€ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ì„ íƒ
# output_cat = st.selectbox(
#     'ì¶”ì²œë°›ê³  ì‹¶ì€ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.',
#     ('top', 'bottom', 'shoes', 'hat', 'sunglasses', 'scarf', 'belt'))
# if input == output:
#     st.error('Error: ì—…ë¡œë“œí•œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ì™€ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.')
# st.write('You selected:', output)
# # ìƒí™© ì¹´í…Œê³ ë¦¬ ì„ íƒ
# situation = st.checkbox(
#     'ìƒí™© ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.',
#     ('ì—¬í–‰', 'ì¹´í˜', 'ì „ì‹œíšŒ', 'ìº í¼ìŠ¤&ì¶œê·¼', 'ê¸‰ì¶”ìœ„', 'ìš´ë™'))
# st.write('You selected:', situation)
# # ì„ íƒëœ ìƒí™© ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ì–´ë¡œ ë³€í™˜í•´ì„œ ë³€ìˆ˜ ì €ì¥
# situation_mapping = {
#     'ì—¬í–‰': 'travel',
#     'ì¹´í˜': 'cafe',
#     'ì „ì‹œíšŒ': 'exhibit',
#     'ìº í¼ìŠ¤&ì¶œê·¼': 'campus_work',
#     'ê¸‰ì¶”ìœ„': 'cold',
#     'ìš´ë™': 'exercise'}
# situation = [situation_mapping[item] for item in situation]


# ì„ì‹œë¡œ ë³€ìˆ˜ ì •ì˜
input_image = './example_top.jpg'
input_cat = 'top'
output_cat = 'bottom'
situation = 'travel'



### ì…ë ¥ë°›ì€ ì´ë¯¸ì§€ segmentation & detection & vectorë³€í™˜ ###
image = Image.open(input_image)

# object detection & cropping í•¨ìˆ˜
def cropping(images,st = 1,
  fi = 0.0,
  step = -0.05):
  image_1 = Image.fromarray(images)
  inputs = processor_obj(images=image_1, return_tensors="pt")
  outputs = model_obj(**inputs)
  for tre in np.arange(st,fi,step):
    try:
        # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)
        target_sizes = torch.tensor([image_1.size[::-1]])
        results = processor_obj.post_process_object_detection(outputs, threshold=tre, target_sizes=target_sizes)[0]
        
        img = None
        for idx, (score, label, box) in enumerate(zip(results["scores"], results["labels"], results["boxes"])):
            box = [round(i, 2) for i in box.tolist()]
            xmin, ymin, xmax, ymax = box
            img = image_1.crop((xmin, ymin, xmax, ymax))

        poss = np.array(img).sum().sum()
        return img
        break
    except:
        continue
  return images

# vector ë³€í™˜ í•¨ìˆ˜
default_path = './'
def image_to_vector(image,resize_size=(256,256)):  # ì´ë¯¸ì§€ size ë³€í™˜ resize(256,256)
    #image = Image.fromarray(image)
    #image = image.resize(resize_size)
    image = Image.fromarray(np.copy(image))
    image = image.resize(resize_size)
    image_array = np.array(image, dtype=np.float32)
    image_vector = image_array.flatten()
    return image_vector

# ì „ì²´ í†µí•© í•¨ìˆ˜
def final_image(image):    
    if len(np.array(image).shape) == 2:
        image = Image.fromarray(image).convert('RGB')
    # segmentation
    inputs = processor_seg(images=image, return_tensors="pt")
    outputs = model_seg(**inputs)
    logits = outputs.logits.cpu()
    upsampled_logits = nn.functional.interpolate(
            logits,
            size=image.size[::-1],
            mode="bilinear",
            align_corners=False,
        )
    pred_seg = upsampled_logits.argmax(dim=1)[0]
    segments = torch.unique(pred_seg)
    default_path = './'
    
    for i in segments:
        if int(i) == 0:
            continue
        if int(i) == 1:
            cloth = 'hat'
            cloths = 'hat'
            mask = pred_seg == i
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif int(i) == 3:
            cloth= 'sunglasses'
            cloths= 'sunglasses'
            mask = pred_seg == i
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif int(i) == 4:
            cloth = 'top'
            cloths = 'top'
            mask = pred_seg == i
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif int(i) in [5,6,7]:
            cloth= ['pants','skirt','dress']
            cloths= 'bottom'
            mask  = (pred_seg == torch.tensor(5)) | (pred_seg == torch.tensor(6)) | (pred_seg == torch.tensor(7))
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif int(i) == 8:
            cloth = 'belt'
            cloths = 'belt'
            mask = pred_seg == torch.tensor(8)
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif (int(i) == 9):
            cloth = 'shoes'
            cloths = 'shoes'
            mask = (pred_seg == torch.tensor(9)) | (pred_seg == torch.tensor(10))
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif int(i) == 16:
            cloth = 'bag'
            cloths = 'bag'
            mask = pred_seg == torch.tensor(16)
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        elif int(i) == 17:
            cloth = 'scarf'
            cloths = 'scarf'
            mask = pred_seg == torch.tensor(17)
            image = np.array(image)
            mask_np = (mask * 255).numpy().astype(np.uint8)
            result = cv2.bitwise_and(image.astype(np.uint8), image.astype(np.uint8), mask=mask_np)
            img = cropping(result)
            img_vector = image_to_vector(img)
        return img_vector
    
# ì…ë ¥ë°›ì€ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì™„ë£Œ
input_img = final_image(image)




### ìœ ì‚¬ë„ ë¶„ì„ ###
# í•˜ë‚˜ëŠ” ì´ë¯¸ì§€, ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ê²½ë¡œë¡œ ë°›ëŠ” ê²½ìš°
def cosine_similarity(vec1, vec2_path):
    vec2 = np.loadtxt(vec2_path)
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    similarity = dot_product / (norm_vec1 * norm_vec2)   
    return similarity
# ë‘˜ ë‹¤ ê²½ë¡œë¡œ ë°›ëŠ” ê²½ìš°
def cosine_similarity_2(vec1_path, vec2_path):
    vec1 = np.loadtxt(vec1_path)
    vec2 = np.loadtxt(vec2_path)
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    similarity = dot_product / (norm_vec1 * norm_vec2)
    return similarity

# ì…ë ¥ë°›ì€ ì´ë¯¸ì§€ & ë™ì¼ ì¹´í…Œê³ ë¦¬ í´ë”ì— ì €ì¥ëœ ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€
sim_list = []
file_path = './style/' + situation + '/' + input_cat + '/'   # ex) './cafe/top/'
cloths = os.listdir('./style/' + situation + '/' + input_cat + '/')
for cloth in cloths:
    sim_list.append(cosine_similarity(input_img, file_path + cloth))
max_idx = np.argmax(sim_list)
cloths[max_idx]
# target_image ì •ì˜
target_image = './style/' + situation + '/' + output_cat + '/' + cloths[max_idx]
# ìœ ì‚¬ë„ ë¶„ì„ ì™„ë£Œëœ ìŠ¤íƒ€ì¼seg ì´ë¯¸ì§€ì™€ product_seg ìœ ì‚¬ë„ë¶„ì„
sim_list = []
file_path = './product/' + output_cat + '/'
cloths = os.listdir('./product/' + output_cat + '/')
for cloth in cloths:
    sim_list.append(cosine_similarity_2(target_image, file_path + cloth))
max_idx = np.argmax(sim_list)
cloths[max_idx]
## ì˜ˆì‹œ ì¶œë ¥ê°’: 'bottom_1883.txt'


# ìµœì¢… ì´ë¯¸ì§€ ì¶œë ¥


# In[6]:


# input_image = './example_top.jpg'
# input_cat = 'top'
# output_cat = 'bottom'
# situation = 'travel'


# In[20]:


# # í•´ë‹¹ url ì°¾ê¸°
# with open('cafe_urls.txt', 'r') as file:
#     urls = file.readlines()
    
# urls[987]


# #### .txt íŒŒì¼ë¡œ ë³€í™˜

# In[21]:


# file_names = os.listdir('../product/bottom_seg')
# file_path = '../product/bottom_seg'
# for name in file_names:
#     src = os.path.join(file_path, name)
#     dst = name + '.txt'
#     dst = os.path.join(file_path, dst)
#     os.rename(src, dst)


# #### ì••ì¶•í•´ì œ

# In[22]:


# # Open the zip file
# zip_file_path = '../product/bottom_seg-20240222T170613Z-001.zip'
# extract_dir = '../product'

# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
#     # Extract all the contents into the specified directory
#     zip_ref.extractall(extract_dir)

